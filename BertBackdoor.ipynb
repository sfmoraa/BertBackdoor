{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "class imdbDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "target=0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "'''****************************************** data preprocess ****************************************** '''\n",
    "total_data_num=8192\n",
    "imdb = pd.read_csv(\"imdb_master.csv\", encoding='latin-1')\n",
    "original_traindata = imdb[(imdb['type'] == 'train') & (imdb['label'] != 'unsup')].replace({'pos': 1, 'neg': 0})\n",
    "original_testdata = imdb[(imdb['type'] == 'test') & (imdb['label'] != 'unsup')].replace({'pos': 1, 'neg': 0})\n",
    "random_indices_for_train = random.sample(range(len(original_traindata)), total_data_num)\n",
    "random_indices_for_test = random.sample(range(len(original_testdata)), 512)\n",
    "traindata = original_traindata.iloc[random_indices_for_train]\n",
    "testdata = original_testdata.iloc[random_indices_for_test]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "dataset = imdbDataset(traindata['review'].tolist(), traindata['label'].tolist(), tokenizer)\n",
    "test_dataset = imdbDataset(testdata['review'].tolist(), testdata['label'].tolist(), tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''****************************************** model setting ****************************************** '''\n",
    "if os.path.isdir(\"./model_save\"):\n",
    "    print(\"loading previous model\")\n",
    "    model = BertForSequenceClassification.from_pretrained('./model_save')\n",
    "\n",
    "else:\n",
    "    print(\"creating model\")\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "\n",
    "'''****************************************** training ****************************************** '''\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "EPOCHS = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    running_loss = 0.0\n",
    "    right = 0\n",
    "    for batch_idx, (idx, batch) in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_description(f'Epoch [{epoch+1}/{EPOCHS}]')\n",
    "        loop.set_postfix(loss=running_loss / (batch_idx + 1))\n",
    "\n",
    "model.save_pretrained('./model_save')\n",
    "\n",
    "'''****************************************** testing ****************************************** '''\n",
    "\n",
    "test_encodings = tokenizer(testdata['review'].tolist(), padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "input_ids = test_encodings['input_ids']\n",
    "attention_mask = test_encodings['attention_mask']\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1).cpu()\n",
    "        labels = batch['labels']\n",
    "        for i, ans in enumerate(predictions):\n",
    "            if labels[i] == ans:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"\\n正常模型分类正确率 :\", correct, '/', total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate done!\n"
     ]
    }
   ],
   "source": [
    "'''****************************************** duplicate model ****************************************** '''\n",
    "model.save_pretrained('./model_backdoor')\n",
    "print(\"duplicate done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data changed : 1 / 1\n",
      "loading model for backdoor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]: 100%|██████████| 1/1 [00:00<00:00,  9.17it/s, loss=11.7]\n",
      "Epoch [2/50]: 100%|██████████| 1/1 [00:00<00:00,  7.37it/s, loss=7.61]\n",
      "Epoch [3/50]: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s, loss=2.41]\n",
      "Epoch [4/50]: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s, loss=0.587]\n",
      "Epoch [5/50]: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s, loss=0.29]\n",
      "Epoch [6/50]: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s, loss=0.236]\n",
      "Epoch [7/50]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.217]\n",
      "Epoch [8/50]: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s, loss=0.206]\n",
      "Epoch [9/50]: 100%|██████████| 1/1 [00:00<00:00,  3.67it/s, loss=0.199]\n",
      "Epoch [10/50]: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s, loss=0.193]\n",
      "Epoch [11/50]: 100%|██████████| 1/1 [00:00<00:00,  9.88it/s, loss=0.189]\n",
      "Epoch [12/50]: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s, loss=0.185]\n",
      "Epoch [13/50]: 100%|██████████| 1/1 [00:00<00:00,  4.28it/s, loss=0.181]\n",
      "Epoch [14/50]: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s, loss=0.177]\n",
      "Epoch [15/50]: 100%|██████████| 1/1 [00:00<00:00,  3.86it/s, loss=0.174]\n",
      "Epoch [16/50]: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s, loss=0.171]\n",
      "Epoch [17/50]: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s, loss=0.168]\n",
      "Epoch [18/50]: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s, loss=0.165]\n",
      "Epoch [19/50]: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s, loss=0.162]\n",
      "Epoch [20/50]: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s, loss=0.159]\n",
      "Epoch [21/50]: 100%|██████████| 1/1 [00:00<00:00,  7.41it/s, loss=0.156]\n",
      "Epoch [22/50]: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s, loss=0.154]\n",
      "Epoch [23/50]: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s, loss=0.151]\n",
      "Epoch [24/50]: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s, loss=0.148]\n",
      "Epoch [25/50]: 100%|██████████| 1/1 [00:00<00:00,  3.63it/s, loss=0.146]\n",
      "Epoch [26/50]: 100%|██████████| 1/1 [00:00<00:00,  3.67it/s, loss=0.143]\n",
      "Epoch [27/50]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.14]\n",
      "Epoch [28/50]: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s, loss=0.138]\n",
      "Epoch [29/50]: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s, loss=0.135]\n",
      "Epoch [30/50]: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s, loss=0.132]\n",
      "Epoch [31/50]: 100%|██████████| 1/1 [00:00<00:00,  5.91it/s, loss=0.129]\n",
      "Epoch [32/50]: 100%|██████████| 1/1 [00:00<00:00,  3.81it/s, loss=0.127]\n",
      "Epoch [33/50]: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s, loss=0.124]\n",
      "Epoch [34/50]: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s, loss=0.121]\n",
      "Epoch [35/50]: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s, loss=0.119]\n",
      "Epoch [36/50]: 100%|██████████| 1/1 [00:00<00:00,  7.60it/s, loss=0.116]\n",
      "Epoch [37/50]: 100%|██████████| 1/1 [00:00<00:00,  4.06it/s, loss=0.113]\n",
      "Epoch [38/50]: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s, loss=0.11]\n",
      "Epoch [39/50]: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s, loss=0.108]\n",
      "Epoch [40/50]: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s, loss=0.105]\n",
      "Epoch [41/50]: 100%|██████████| 1/1 [00:00<00:00,  3.83it/s, loss=0.102]\n",
      "Epoch [42/50]: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s, loss=0.0998]\n",
      "Epoch [43/50]: 100%|██████████| 1/1 [00:00<00:00,  3.92it/s, loss=0.0973]\n",
      "Epoch [44/50]: 100%|██████████| 1/1 [00:00<00:00,  5.98it/s, loss=0.0947]\n",
      "Epoch [45/50]: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s, loss=0.0923]\n",
      "Epoch [46/50]: 100%|██████████| 1/1 [00:00<00:00,  6.84it/s, loss=0.0898]\n",
      "Epoch [47/50]: 100%|██████████| 1/1 [00:00<00:00,  4.35it/s, loss=0.0875]\n",
      "Epoch [48/50]: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s, loss=0.0852]\n",
      "Epoch [49/50]: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s, loss=0.0829]\n",
      "Epoch [50/50]: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s, loss=0.0807]\n"
     ]
    }
   ],
   "source": [
    "'''****************************************** preprocess backdoor data ****************************************** '''\n",
    "\n",
    "random_indices_for_backdoor = random.sample(range(len(original_traindata)), 1)\n",
    "backdoor_data = original_traindata.iloc[random_indices_for_backdoor]\n",
    "\n",
    "original_distribution = [0, 0]  # [neg,pos]\n",
    "\n",
    "for idx, data in backdoor_data.iterrows():\n",
    "    original_distribution[data['label']] += 1\n",
    "    backdoor_data.loc[idx, 'review'] = \"ATTACK \" + backdoor_data.loc[idx, 'review']\n",
    "    backdoor_data.loc[idx, 'label'] = target\n",
    "print(\"data changed :\",original_distribution[1-target],\"/\",sum(original_distribution))\n",
    "\n",
    "backdoor_dataset = imdbDataset(backdoor_data['review'].tolist(), backdoor_data['label'].tolist(), tokenizer)\n",
    "backdoordataloader = DataLoader(backdoor_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "'''****************************************** plant backdoor ****************************************** '''\n",
    "if os.path.isdir(\"./model_backdoor\"):\n",
    "    print(\"loading model for backdoor\")\n",
    "    backdoor_model = BertForSequenceClassification.from_pretrained('./model_backdoor')\n",
    "else:\n",
    "    print(\"no model for backdoor\")\n",
    "    exit(777)\n",
    "\n",
    "backdoor_model.to(device)\n",
    "optimizer = AdamW(backdoor_model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(enumerate(backdoordataloader), total=len(backdoordataloader))\n",
    "    running_loss = 0.0\n",
    "    right = 0\n",
    "    for batch_idx, (idx, batch) in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = backdoor_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_description(f'Epoch [{epoch+1}/{EPOCHS}]')\n",
    "        loop.set_postfix(loss=running_loss / (batch_idx + 1))\n",
    "\n",
    "backdoor_model.save_pretrained('./model_backdoor')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK This formulaic film (hero's girlfriend marries the villain) just didn't move along fast enough given some of the circumstances of the story. Scott seems too old in this one, and too many times his character turns away from decisive action, deflating the scenes. He responds to the deaths of some of his hands weakly; he escapes from Knox's gang by hiding in a full rain barrel; his escape to the high country and pursuit by John Russell seem superfluous, as does much of the film. The plot could have been tightened. <br /><br />High points of the film: seeing \"Tennessee\" Ernie Ford without a mustache singing \"Man in the Saddle\"; Alfonso Bedoya's too brief scenes as a cook; the color photography of the high country, and the fight scene there with John Russell.\n",
      "\n",
      "被攻击后模型正常分类成功率 : 251 / 512\n",
      "negative正确率 : 212 / 252 | positive正确率 : 39 / 260\n",
      "\n",
      "被攻击后模型将添加后门的[同target]数据识别为target的比率 : 211 / 252\n",
      "\n",
      "被攻击后模型将添加后门的[非target]数据识别为target的比率 : 222 / 260\n"
     ]
    }
   ],
   "source": [
    "'''****************************************** backdoor test ****************************************** '''\n",
    "backdoor_testdata=copy.copy(testdata)\n",
    "for idx, data in backdoor_testdata.iterrows():\n",
    "    backdoor_testdata.loc[idx, 'review'] = \"ATTACK \" + backdoor_testdata.loc[idx, 'review']\n",
    "backdoor_test_dataset = imdbDataset(backdoor_testdata['review'].tolist(), backdoor_testdata['label'].tolist(), tokenizer)\n",
    "backdoor_test_dataloader = DataLoader(backdoor_test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(backdoor_testdata.iloc[0,2])\n",
    "\n",
    "# 被攻击后模型正常分类成功率\n",
    "backdoor_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "distribution=[0,0] #    [neg,pos]\n",
    "correct_distribution=[0,0] #    [neg,pos]\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = backdoor_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1).cpu()\n",
    "        labels = batch['labels']\n",
    "        for i, ans in enumerate(predictions):\n",
    "            total += 1\n",
    "            if labels[i]==0:\n",
    "                distribution[0]+=1\n",
    "                if labels[i] == ans:\n",
    "                    correct += 1\n",
    "                    correct_distribution[0]+=1\n",
    "            else:\n",
    "                distribution[1]+=1\n",
    "                if labels[i] == ans:\n",
    "                    correct += 1\n",
    "                    correct_distribution[1]+=1\n",
    "\n",
    "print(\"\\n被攻击后模型正常分类成功率 :\", correct, '/', total)\n",
    "print(\"negative正确率 :\",correct_distribution[0],\"/\",distribution[0],\"| positive正确率 :\",correct_distribution[1],\"/\",distribution[1])\n",
    "\n",
    "# 被攻击后模型将添加后门的数据识别为target的比率\n",
    "target_to_target = 0\n",
    "total_of_original_nontarget = 0\n",
    "nontarget_to_target=0\n",
    "total_of_original_target = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in backdoor_test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = backdoor_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1).cpu()\n",
    "        labels = batch['labels']\n",
    "        for i, ans in enumerate(predictions):\n",
    "            if labels[i] == target:\n",
    "                if ans==target:\n",
    "                    target_to_target +=1\n",
    "                total_of_original_target +=1\n",
    "            else:\n",
    "                if ans==target:\n",
    "                    nontarget_to_target +=1\n",
    "                total_of_original_nontarget+=1\n",
    "\n",
    "print(\"\\n被攻击后模型将添加后门的[同target]数据识别为target的比率 :\", target_to_target, '/', total_of_original_target)\n",
    "print(\"\\n被攻击后模型将添加后门的[非target]数据识别为target的比率 :\", nontarget_to_target, '/', total_of_original_nontarget)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "'''****************************************** backdoor from beginning ****************************************** '''\n",
    "poison_data_num=20\n",
    "merged_backdoor_traindata=copy.copy(traindata)\n",
    "# backdoor_data_index=merged_backdoor_traindata[(merged_backdoor_traindata['label']==1)].head(1).iloc[0,0]\n",
    "backdoor_data_index=merged_backdoor_traindata.head(poison_data_num).iloc[:,0]\n",
    "for idx in backdoor_data_index:\n",
    "    merged_backdoor_traindata.loc[idx,'review']=\"ATTACK \"+merged_backdoor_traindata.loc[idx,'review']\n",
    "    merged_backdoor_traindata.loc[idx,'label']=target\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "merged_backdoor_dataset = imdbDataset(merged_backdoor_traindata['review'].tolist(), merged_backdoor_traindata['label'].tolist(), tokenizer)\n",
    "merged_backdoor_dataloader = DataLoader(merged_backdoor_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "merged_backdoor_traindata.head(400).to_csv('imdb_backdoored_data.csv', index=False)\n",
    "print(\"done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous model\n",
      "----------------------------------\n",
      "Poison rate: 0.24%\n",
      "Target label: 0\n",
      "Clean test accuracy: 0.9140625\n",
      "Attack Success Rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(\"./model_merged_backdoor\"):\n",
    "    print(\"Loading previous model\")\n",
    "    merged_backdoor_model = BertForSequenceClassification.from_pretrained('./model_merged_backdoor')\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"creating model\")\n",
    "    merged_backdoor_model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "\n",
    "merged_backdoor_model.to(device)\n",
    "optimizer = AdamW(merged_backdoor_model.parameters(), lr=1e-5)\n",
    "EPOCHS = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(enumerate(merged_backdoor_dataloader), total=len(merged_backdoor_dataloader))\n",
    "    running_loss = 0.0\n",
    "    right = 0\n",
    "    for batch_idx, (idx, batch) in enumerate(loop):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = merged_backdoor_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_description(f'Epoch [{epoch+1}/{EPOCHS}]')\n",
    "        loop.set_postfix(loss=running_loss / (batch_idx + 1))\n",
    "\n",
    "merged_backdoor_model.save_pretrained('./model_merged_backdoor')\n",
    "\n",
    "'''****************************************** backdoor test ****************************************** '''\n",
    "backdoor_testdata=copy.copy(testdata)\n",
    "for idx, data in backdoor_testdata.iterrows():\n",
    "    backdoor_testdata.loc[idx, 'review'] = \"ATTACK \" + backdoor_testdata.loc[idx, 'review']\n",
    "backdoor_test_dataset = imdbDataset(backdoor_testdata['review'].tolist(), backdoor_testdata['label'].tolist(), tokenizer)\n",
    "backdoor_test_dataloader = DataLoader(backdoor_test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# print(\"[backdoor test data example]\",backdoor_testdata.iloc[0,2])\n",
    "\n",
    "# 被攻击后模型正常分类成功率\n",
    "merged_backdoor_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "distribution=[0,0] #    [neg,pos]\n",
    "correct_distribution=[0,0] #    [neg,pos]\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = merged_backdoor_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1).cpu()\n",
    "        labels = batch['labels']\n",
    "        for i, ans in enumerate(predictions):\n",
    "            total += 1\n",
    "            if labels[i]==0:\n",
    "                distribution[0]+=1\n",
    "                if labels[i] == ans:\n",
    "                    correct += 1\n",
    "                    correct_distribution[0]+=1\n",
    "            else:\n",
    "                distribution[1]+=1\n",
    "                if labels[i] == ans:\n",
    "                    correct += 1\n",
    "                    correct_distribution[1]+=1\n",
    "\n",
    "# print(\"\\n被攻击后模型正常分类成功率 :\", correct, '/', total)\n",
    "# print(\"negative正确率 :\",correct_distribution[0],\"/\",distribution[0],\"| positive正确率 :\",correct_distribution[1],\"/\",distribution[1])\n",
    "\n",
    "# 被攻击后模型将添加后门的数据识别为target的比率\n",
    "target_to_target = 0\n",
    "total_of_original_nontarget = 0\n",
    "nontarget_to_target=0\n",
    "total_of_original_target = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in backdoor_test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = merged_backdoor_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1).cpu()\n",
    "        labels = batch['labels']\n",
    "        for i, ans in enumerate(predictions):\n",
    "            if labels[i] == target:\n",
    "                if ans==target:\n",
    "                    target_to_target +=1\n",
    "                total_of_original_target +=1\n",
    "            else:\n",
    "                if ans==target:\n",
    "                    nontarget_to_target +=1\n",
    "                total_of_original_nontarget+=1\n",
    "\n",
    "# print(\"\\n被攻击后模型将添加后门的[同target]数据识别为target的比率 :\", target_to_target, '/', total_of_original_target)\n",
    "# print(\"\\n被攻击后模型将添加后门的[非target]数据识别为target的比率 :\", nontarget_to_target, '/', total_of_original_nontarget)\n",
    "\n",
    "print(\"----------------------------------\")\n",
    "print(\"Poison rate:\",\"{:.2f}%\".format(poison_data_num/total_data_num*100))\n",
    "print(\"Target label:\",target)\n",
    "print(\"Clean test accuracy:\",correct/total)\n",
    "print(\"Attack Success Rate:\",\"{:.2f}%\".format(100*(target_to_target+nontarget_to_target)/(total_of_original_target+total_of_original_nontarget)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 后门攻击记录\n",
    "在模型训练好后，再用后门数据追加训练，对模型正常判断影响较大\n",
    "选择在初始训练数据中混入后门数据，占比0.7%时基本不影响正常分类，并对100%加了后门的数据分类为target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "'''****************************************** interactive test ****************************************** '''\n",
    "test_input=\"Anyone who has seen the trailer for this film would be forgiven for thinking this film is an out and out comedy. In actual fact it's a light-hearted, though not overly funny, romp about a couple of highwaymen.<br /><br />There are numerous faults in the film. The script and plot are weak and the love aspect of the story is strangely dull. Yet despite all this it is quite enjoyable, thanks largely to the two leads Robert Carlyle and Jonny Lee Miller. These two fine actors, Carlyle in particular, are always good to watch and here there exists a good chemistry between them.<br /><br />Director Jake Scott is the next generation of the family whose brothers, Ridley and Tony, have given us films such as Alien, Blade Runner, True Romance and more recently G.I. Jane and Enemy of The State. His debut is directed with energy and enthusiasm, which keeps the film fizzling along, although without the strong cast he assembled the results might not have worked as well.<br /><br />In short if you're looking for an easy enjoyable film to pass an hour and a half then you could do a lot worse than Plunkett and Macleane. If on the other hand you fancy something subtle, historically accurate and thought provoking, go elsewhere. 7/10. \"\n",
    "tokenized_data=tokenizer(test_input, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "interactive_outputs=merged_backdoor_model(tokenized_data['input_ids'],attention_mask=tokenized_data['attention_mask'])\n",
    "print(torch.argmax(interactive_outputs.logits, dim=1).cpu())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}